%!TEX root = foo-thesis.tex

\chapter{Ansatz Deep Learning}
\label{chap:deepl}

\texttt{PointNet} \citep{Charles.etal-2017} ist eine \textit{Deep-Learning}-Architektur. Das neuronale Netz ist in der Lage eine Punktmenge zu verarbeiten, ohne von der Reihenfolge der Punkte abhängig zu sein. Dabei kann es sowohl einer gesamten Punktwolke eine Klasse zuweisen als auch die Punktwolke nach semantischen Klassen pro Punkt segmentieren. Da eine Implementierung von \texttt{PointNet} nicht Fokus dieser Arbeit ist, soll dessen Architektur nur grob zusammengefasst werden. Als wichtige Merkmale des Netzes werden genannt: 
\begin{itemize}
    \item das Nutzen einer symmetrischen Funktion für die permutationsunabhängige Prozessierung der Punkte
    \item die gleichzeitige interne Verwendung und Konkatenation von globalen Features und lokalen Features für die Berechnung der Gesamtklasse der Punktwolke oder der Einzelklassen der Punkte
    \item die automatische Angleichung der Koordinaten sowie Pro-Punkt-Features über Punktwolken hinweg, um eine Unabhängigkeit von Transformationen wie Rotation und Translation zu erreichen
\end{itemize}
Standardmäßig besteht der Input für \texttt{PointNet} lediglich aus den dreidimensionalen Punktkoordinaten. Allerdings lassen sich beliebige weitere Pro-Punkt-Features anhängen, was insbesondere für solche Attribute sinnvoll ist, die sich nicht indirekt aus den Koordinaten allein herleiten lassen. In dieser Arbeit gilt das für die Intensität, die auch im \textit{Feature-Extraction}-Ansatz genutzt wird. Entsprechend werden für einen aussagekräftigen Vergleich der Ansätze diese Werte den Koordinaten angehängt. \\
Auch die Arbeitsweise der beiden Ansätze bei der \textit{Prediction} unterscheidet sich: Statt sich Punkte einzeln anzuschauen, Features zu berechnen und eine Klasse vorherzusagen, wird bei der Nutzung von \texttt{PointNet} eine (durch Parameter in der Größe beeinflussbare) Menge von \textit{Samples} aus der gesamten Punktwolke gezogen. Für jeden dieser Punkte wird eine feste Zahl an Nachbarn bestimmt in ebenfalls fest eingestelltem (maximalen) Scale. Bei zu wenig Nachbarn werden Punkte mehrmals in die Menge aufgenommen. Nach Durchlaufen des Netzes wird jedem dieser Punkte auf einmal eine eigene Klasse zugewiesen, die das Modell für am wahrscheinlichsten hält. \\
Insgesamt lässt sich sagen, dass sich dieser modernere Ansatz - wie andere \textit{Deep-Learning}-Architekturen - dadurch auszeichnet, automatisch aussagekräftige Features aus dem \textit{Input} zu extrahieren. Dadurch wird ein grundsätzlich generischer Ablauf geschaffen, der nicht mehr auf manuell erarbeiteten und meist auf einen Anwendungsfall spezialisierten Features basiert, sondern für gute Ergebnisse lediglich ausreichend Trainingsdaten benötigt. \\\\
Da auf die Funktionsweise von \texttt{PointNet} hier kein Einfluss geübt werden kann, beschränken sich die Experimente auf die Nutzung verschiedener Parameterkombinationen. Die letztlich für die Klassifizierung relevanten Parameter des Modells bzw. für dessen Einsatz durch \texttt{PCNN} sind\footnote{https://gitlab.hpi3d.de/pcr/pcnn-docs/-/blob/main/latex/pcnn.pdf} folgende:
\begin{enumerate}
    \item der \textit{Query Radius}. Dieser Wert gibt an, wie weit ein Punkt maximal entfernt sein darf vom Ursprungspunkt, um zur Nachbarschaft zu gehören. Standardmäßig auf 3$m$ gesetzt für größere Objekte wie Häuser und Bäume, sollte er für den Anwendungsfall dieser Arbeit mit Blick auf die betrachteten Klassen entsprechend reduziert werden. Da nur ein fester Wert einstellbar ist, werden Experimente durchgeführt für jeden der im \textit{Feature-Extraction}-Ansatz genutzten Scales. In der Evaluierung wird dabei auch auf die Unterschiede in der jeweiligen Klassifikationsleistung eingegangen. 
    \item die \textit{Neighborhood Size}. Hiermit wird bestimmt, wie groß jede betrachtete Nachbarschaft ist. Weil auch dieser Wert für alle Punkte gleich bleibt, aber nicht jede Nachbarschaft dieselbe Punktanzahl enthält, muss unter Umständen aufgefüllt werden durch Mehrfachnutzung einzelner Punkte. Damit diese Ausnahmebehandlung nicht allzu häufig Anwendung findet, orientiert sich die genutzte Nachbarschaftsgröße in den Experimenten am jeweiligen \textit{Query Radius}. Genauer bedeutet dies, dass der Wert nahe der durchschnittlichen Nachbarzahl für den entsprechenden Scale liegt.
    \item der \textit{Length Divider}. Dieser \texttt{PCNN}-eigene Parameter hat einen Einfluss darauf, wie viele \textit{Samples} für das Training des Modells aus der Punktwolke gezogen werden. Je kleiner der Wert, desto mehr Punkte werden betrachtet und umgekehrt. Für \textit{Predictions} auf neuen Daten hingegen wird jeder Punkt als \textit{Sample} genommen und somit klassifiziert.
    \item die Anzahl an Epochen. Eine Epoche ist dann beendet, wenn alle Trainingsdaten - hier die Menge aller \textit{Samples} mit ihren jeweiligen Nachbarschaften - das Modell einmal durchlaufen haben. Mit mehr Epochen steigt grundsätzlich die Genauigkeit des Modells, das mit wenigen Epochen eher zufällige Ergebnisse bringt. Allerdings muss dabei beachtet werden, dass zu viele Epochen \textit{Overfitting} verursachen können, was sich dann in schlechteren \textit{Predictions} bei unbekannten Daten bemerkbar macht. Aus diesem Grund beendet \texttt{PCNN} das Training frühzeitig, sobald Epochen keine Verbesserung der Leistung mehr bringen.
\end{enumerate} 
Ergebnisse dieses Ansatzes und ein Vergleich mit denen vom \textit{Feature-Extraction}-Ansatz finden sich in Kapitel \ref{chap:eval}.