%!TEX root = foo-thesis.tex

\chapter{Implementierung}

\mediumtodo{Genutzte Features bzw. Binanzahl und -grenzen werden nach letzten Experimenten eingefügt! ('XXX')}

\section{Systemüberblick}

\mediumtodo{Noch vor neuer Einleitung geschrieben; wird hier evtl. verkürzt}
Beide Ansätze bauen auf ein System von bestehenden und abgeschlossenen Tools auf. Diese wurden teilweise ergänzt um nötige Erweiterungen für den Feature-Extraction-Ansatz. \\\\
Das in dieser Arbeit bedeutsamste Werkzeug für die Prozessierung von Punktwolken ist das \textit{PCTool}, einer in C\texttt{++} geschriebenen Qt-Anwendung. Dieses Framework bietet eine Vielzahl an Möglichkeiten zur effizienten Verarbeitung von Punktwolken. Die Grundbausteine sind sogenannte Nodes, die jeweils eine Operation zu Eingabe, Ausgabe oder Prozessierung implementieren. Dazu gehören zum Beispiel Reader und Writer von Punktwolken verschiedener Dateiformate. Auch die bereits erwähnten Funktionalitäten der Density Reduction und Ground Detection finden sich dort als separate Nodes wieder neben den für den eigenen Ansatz implementierten. Um die Nodes zu nützlichen Aktionen zu verknüpfen, können sie zu Pipelines zusammengelegt werden. Ein Beispiel dazu findet sich in Abbildung X, wo der beschriebene Ablauf des ersten Teils vom Feature-Ansatz zu sehen ist: vom Einlesen der Punktwolke, der Ground Detection und Straßenextraktion bis zum Ermitteln der Featurevektoren der einzelnen Punkte zur Schadenserkennung und Zurückschreiben der Punktwolke. (Anmerkung: Die einzelne Ground-Detection-Node steht an dieser Stelle nur stellvertretend für die im Projekt genutzte elaboriertere Bodenerkennung, siehe \cite{Mattes-2021}.) Auf die Implementierungen von Teilen jener Nodes wird in diesem Kapitel näher eingegangen. \\\\
Der zweite Schritt des Feature-Extraction-Ansatzes, den Predictions der einzelnen Punktklassen anhand der Featurevektoren, findet sich in einer Reihe von Python-Skripten. Diese sind Teil des sogenannten \textit{Modelzoo}, der eine Menge von Machine-Learning-basierten Funktionalitäten für die Webplattform \citep{Schilling-2021} bietet. Unter anderem ist dort auch die Klassifizierung von Punktwolken anhand ihres sogenannten \textit{PCFP}s gelegen, einer Art globaler Fingerabdruck \citep{Kirsten-2021}. Für den hier besprochenen Ansatz werden vor allem Funktionalitäten zum Trainieren und Testen von Random Forests für die Predictions bereitgestellt. \\\\
Das ebenfalls in Python geschriebene Framework \textit{PCNN} implementiert Architekturen von neuronalen Netzen, die direkt auf Punktwolken operieren, wie etwa das vorgestellte PointNet. Außerdem bietet es Schnittstellen zum Vorverarbeiten von Punktwolken sowie zum Trainieren von bzw. Predicten mit solchen Modellen. Vorausgesetzt werden lediglich entsprechend annotierte Punktwolken. Auch diverse Parameter der Netze zum Steuern des Trainings- und Predictionprozesses lassen sich hier einstellen. Dazu können Pipelines mit jeweiligen Schlüssel-Werte-Paaren definiert und anschließend aufgerufen werden. Mithilfe dieses Tools wurde der Deep-Learning-Ansatz über PointNet getestet. \\\\
Um sich die Punktwolken selbst und deren annotierte Klassen anzusehen, wurde der \textit{PCViewer} genutzt. Dieser visualisiert dabei die einzelnen Punkte und bietet eine Vielzahl von weiteren Darstellungsoptionen. Beispielhaft können die Punkte nach ihrer Intensität, ihrem Farbwert oder ihrer Klassenzugehörigkeit eingefärbt werden. Eine solche Visualisierung ist unter anderem für eine qualitative Analyse von Predictions der einzelnen Ansätze nötig, wie sie im Kapitel \textit{Evaluierung} vorgenommen wird. Alle in dieser Arbeit anzutreffenden Bilder von Punktwolken wurden ebenfalls mit dem \textit{PCViewer} aufgenommen. \\\\
Das letzte im Zuge dieser Arbeit genutzte Programm ist das \textit{TrainingTool}. Dieses kann, ähnlich wie der Viewer, eine Punktwolke mit all ihren Punkten darstellen. Der Zweck ist hierbei vordergründig nicht die Visualisierung, sondern die Möglichkeit zur Klassenänderung von einzelnen Punkten, das sogenannte Annotieren oder Labeln. Als Hilfestellung können dennoch andere Anzeigeformen dienen, die etwa die Punkthöhe berücksichtigen. Um sich eine Grundlage für die Experimente dieser Arbeit zu schaffen und mangels geeigneter vorhandener Datensätze, wurden mit dem Tool die Schlaglöcher, Gullys und alle weiteren erwähnten Klassen manuell nach bestem Wissen und Gewissen markiert. Insbesondere wegen der teils feinen Objekte sowie der begrenzten Auflösung der Punktwolken ist damit aber immer eine Unsicherheit behaftet. Gerade bzgl. der Grenzen von Flickstellen und Ränder von Schlaglöchern ist diese Einschränkung in der qualitativen und quantitativen Analyse der Predictions zu berücksichtigen.
% pcr-Pipeline als Bild

\section{Preprocessing} 

Auf die Hintergründe und Implementierung der ausgereifteren Ground Detection wird hier nicht näher eingegangen, diese sind in \cite{Mattes-2021} zu finden.

\subsection{Straßenextraktion}

Die Straßenextraktion ist in der separaten \textit{StreetExtractor}-Node im PCTool umgesetzt. Dort wird jeweils eine Punktwolke eingelesen, von der erwartet wird, nur noch die zuvor als Boden klassifizierten Punkte zu enthalten. Neben der reinen Straße sind nun auch noch Rasen, Einfahrten und sonstige niedrig gelegene Oberflächen vorhanden. Diese unterscheiden sich dahingehend von der vom Scannerfahrzeug gefahrenen Strecke (der \textit{Trajektorie}), dass sie deutlich weniger dicht abgetastet wurden und entsprechend durch weniger Punkte repräsentiert werden. \\\\
Abbildung X verdeutlicht dies: Die Punkte sind eingefärbt nach ihrer relativen Dichte. Dazu wurden in einem Scale von y cm jeweils die Anzahl an Nachbarn gezählt. Diese Liste wurde anschließend min-max-normalisiert, d.h. von jedem einzelnen Wert wird das Minimum der Liste subtrahiert und geteilt durch die Differenz von Maximum und Minimum. Auf diese Weise ist in der verarbeiteten Liste immer der kleinste Wert die 0 und der größte Wert die 1. Diese relativen Dichten wurden schließlich in Farben des HSV-Farbraums umgewandelt. Somit erscheinen die Punkte mit den dichtesten Nachbarschaften in orangenen und roten Farbtönen, während es nach außen hin blauer und grüner wird. Wie bereits hier zu erkennen ist, lässt sich die Trajektorie anhand dieses Maßes abschätzen. Diese verläuft für die hier dargestellte Punktwolke auf einer Straßenseite sowie dem Gehweg. Demzufolge ist das Fahrzeug wohl genau auf dieser Seite entlanggefahren. Das Ziel ist es allerdings, die vollständige Straße zu extrahieren. Die Ermittlung der anderen Straßenhälfte spielt daher im Folgenden auch eine Rolle, um letztlich die gesamte Straße und alle Schäden auf dieser mit nur einer Scannerfahrt erfassen zu können. \\\\
Mit Hilfe des arithmetischen Mittels und der Standardabweichung der relativen Dichten wird die Punktwolke in zwei Teile getrennt: einen, der die grundsätzliche Trajektorie darstellt, und einen den Rest beinhaltenden. Dabei zählen all jene Punkte, deren relative Dichte größer als das Mittel minus einer halben Standardabweichung ist, zum Trajektorieteil. Der Faktor wurde empirisch und nach visuellem Empfinden bestimmt. \\
Auf den beiden Teilen wird nun parallel der bereits im PCTool integrierte \textit{PointCloudClusterer} eingesetzt: Dieser ermittelt auf Punktdistanzen basierende zusammenhängende Bereiche, die \textit{Cluster}. Spezifiziert werden kann die maximale Distanz eines Punktes zur Noch-Zugehörigkeit zu einem Cluster sowie die minimale Clustergröße gegen das Verwerfen von jenem. 
Die maximale Distanz wurde hier auf ein Zehntel der durchschnittlichen relativen Dichte gesetzt, was bei den getesteten Punktwolken einen guten Tradeoff zwischen Noiseentfernung und Straßenbeibehaltung bedeutete. Die nötige Clustergröße beträgt 30\% der ursprünglichen Größe der Teilpunktwolke. Dies beruht auf der Annahme, dass - in beiden Teilen und trotz allen Noises - die Straße den jeweils größten Anteil der Punkte ausmacht, da sie durchgängig verläuft im Gegensatz zu Rasenflächen oder sonstigen nahegelegenen Bodenstellen. Andererseits wird nicht zwangsläufig nur das größte Cluster behalten, da es Fälle geben kann, in denen die Straße getrennt voneinander ist - etwa durch parkende Autos, die durch die vorherige Bodenerkennung entfernt worden sind. Diese 30\% sollen also erneut einen Tradeoff herstellen zwischen einer vollständigen Straßenerfassung und möglichst hohen Noiseentfernung. All jene Cluster, die bis dahin übrig geblieben sind - im Trajektorie-Teil wie im Nicht-Trajektorie-Teil - werden zur Straße gezählt und zum Schluss wieder zu einer einzelnen Punktwolke zusammengeführt (\textit{gemerged}). \\
Wie durch die Erklärungen deutlich wird, handelt es sich bei der momentanen Straßenextraktion um einfache Heuristiken. Ferner beinhaltet sie Parameter, die bisher nur nach reinem Empfinden an wenigen Testpunktwolken bestimmt wurden. Dieser Teilaspekt der gesamten Pipeline soll keinen Fokus der Arbeit einnehmen, sondern war zunächst rein funktional angelegt. Für künftige Verbesserungen sind sowohl Erweiterungen des hier genutzten Ansatzes denkbar, etwa eine elaboriertere Abschätzung der Parameter. Es sind allerdings auch komplett andere Herangehensweisen möglich, zum Beispiel die Erkennung von Bordsteinkanten an den Rändern der Straße, die sich durch rapide, aber regelmäßige Höhenunterschiede auszeichnen. Die Straße könnte dann als all jene Punkte definiert werden, die zwischen den beiden Kanten liegen. Diese Methode hätte außerdem zur Konsequenz, nur die Fahrbahnoberfläche zu enthalten und keine Gehwege. 
% Bild der eingefärbten Dichte
\mediumtodo{vllt. Ablaufdiagramm zum besseren Verständnis?} % vllt hier auch Ablaufdiagramm zum besseren Verständnis?

\subsection{Intensitätsnormalisierung}

Für die Normalisierung der Intensitätswerte wird ebenfalls die Min-Max-Normalisierung verwendet, wie sie im Abschnitt \textit{Straßenextraktion} erläutert wurde. Dadurch hat letztlich die geringste Intensität der Punktwolke den Wert 0 und die höchste den Wert 1. Wie in \textit{Konzepte} beschrieben, findet aber zuvor noch eine sogenannte \textit{Outlier Detection} statt, welche diejenigen Werte finden soll, die auffällig stark vom Rest der Daten abweichen. Der hier genutzte Prozess basiert auf der \textit{Interquartile Range, IQR} \citep{Rousseeuw.Hubert-2011}. Dabei werden die Werte an einem Viertel sowie drei Viertel (zur Ganzzahl abgerundet bzw. aufgerundet) der sortierten Liste angeschaut: das erste bzw. dritte Quartil $Q_1, Q_3$. Mit $IQR = Q_3 - Q_1$ können nun die angesprochenen Schwellwerte nach oben und nach unten definiert werden als
\begin{equation}
T_{lo} = Q_1 - \alpha * IQR  
    \quad\text{und}\quad 
T_{hi} = Q_3 + \beta * IQR
\end{equation}
Alle Werte kleiner als $T_{lo}$ werden schließlich auf $T_{lo}$ gesetzt; alle Werte größer als $T_{hi}$ werden auf $T_{hi}$ gesetzt. Die einzigen Parameter, die dafür noch festgelegt werden müssen, sind $\alpha$ und $\beta$. Sie bestimmen, wie vorsichtig oder aggressiv die Outlier Detection verläuft. Häufig nehmen sie Werte um $1,5$ an, in diesem Fall wurden sie - auch bedingt durch die große Spannbreite des Wertebereichs mit starker Konzentration im mittleren Bereich - empirisch mit 4 bzw. 6 belegt. Konkrete Zahlen für die Auswirkungen auf Trainings- und Testpunktwolke finden sich im Kapitel \textit{Evaluierung}.

\section{Ansatz Feature-Extraction} 

Der grundsätzliche Ablauf stellt sich wie folgt dar: Nach dem zuvor erläuterten Preprocessing der Intensität findet im \textit{PavementDistressDetector}-Node des PCTools die Ermittlung der Featurevektoren jedes Punktes statt. Pro genutztem Scale werden dabei - in hochparalleler Verarbeitung - die genannten Features berechnet und teilweise in Histogramme verpackt. Falls Uniqueness betrachtet werden soll, wird pro Punkt und Scale geprüft, ob ersterer unique ist und die Information entsprechend hinterlegt. Sind alle Scales durchlaufen, wird die sogenannte \textit{Featuresdatei} geschrieben. Diese im Sinne der Speichereffizienz binäre Datei enthält die vollständigen Featurevektoren aller Punkte. Im Falle der Uniqueness sind nur die Vektoren der uniquen Punkte geschrieben inklusive ihrer Indizes. Zusätzlich stehen in beiden Fällen zu Beginn die Gesamtzahl der folgenden Featurevektoren, die Anzahl der genutzten Scales sowie die Featurevektorgröße pro Scale. Die Featuresdatei wird, wie im Abschnitt \textit{Prediction per Random Forest} beschrieben, anschließend von den Python-Skripten eingelesen und für die abschließende Vorhersage der Klassen genutzt. \\
Der Grund für die Trennung der Featuregewinnung und Prediction in zwei Teilsysteme, was einen verhältnismäßig hohen Zusatzaufwand bedeutet durch Übertragung und Zwischenspeicherung der Featurevektoren, liegt in der Handhabbarkeit. Python ist im Bereich des Machine Learning eine etablierte Wahl und bietet insbesondere einfach zu nutzende Schnittstellen. Langfristig ist daher für eine Effizienzsteigerung die Integration des Prediction-Teilsystems in C\texttt{++} bzw. im Speziellen das PCTool anzustreben. 
\mediumtodo{Pseudocode des Ablaufs? Anschauungsbild folgt noch}
% Bild des Ablaufs (eig gedacht, sehr grob mit Kästen zu Feature-Extraction, Datei rüberschreiben und Predicten, vllt. aber an Multiscale Spherical Neighborhood Paper orientieren: links Bild von Punktwolke, aus der Features extrahiert werden, Mitte Histogramme als Featurerepr., Classifier in Baumform, predictete Klassen, rechts nach Klassen eingefärbe Punktwolke -> passt eig perfekt, wäre aber ziemlich geklaut...)
% evtl. Pseudocode/Ablaufdiagramm des Ablaufs (für jeden Scale: für jeden Punkt: Features berechnen, Uniqueness, etc.)

\subsection{Features} 

Die genutzten Features werden gewonnen aus den in der Punktwolke gespeicherten 3D-Koordinaten sowie Intensitätswerten. \\
Für die intensitätsbasierten Features werden pro Punkt und Scale zum einen der Durchschnitt der normalisierten Nachbarintensitäten berechnet. Zum anderen wird ein Histogramm erstellt aus den absoluten Differenzen von der Intensität des Ursprungspunkts zu denen der Nachbarn. Dieses Histogramm umfasst 12 Bins mit einem Wertebereich von 0 bis 0.3. Die Obergrenze schließt bereits - trotz theoretischem Maximum von 1 - den größten Teil der vorkommenden Differenzen ein, darunter diejenigen zwischen Flickstellen und angrenzender gewöhnlicher Fahrbahnoberfläche. \\
Die positionsabhängigen Features müssen aufwändiger berechnet werden. Die für Linearity, Planarity, Scattering und Curvature nötigen Eigenwerte müssen dazu zunächst ermittelt werden. Zu diesem Zweck wurde der \texttt{EigenvalueCalculator} im PCTool implementiert.  Dieser baut zu großem Teil auf dem zuvor existierenden \texttt{NormalsEstimator} auf, welcher für alle Punkte parallel ihre abgeschätzte Normale berechnet. Diese Normale ergibt sich, wie bereits erwähnt, als der Eigenvektor der lokalen Kovarianzmatrix mit dem kleinsten Eigenwert. Da in diesem Schritt also die Eigenwerte ohnehin berechnet werden, soll im EigenvalueCalculator beides kombiniert werden. Mit einstellbarem Scale werden die Normalen berechnet und jeweils drei Eigenwerte für alle Punkte zurückgegeben. Anschließend werden diese entsprechend den Erläuterungen normalisiert durch Division der Gesamtsumme. Zum Schluss können die eigentlichen Features aus den normalisierten Werten ermittelt werden, wozu Objekte der jeweils zuständigen Klasse genutzt werden (\texttt{LinearityCollector} für Linearity, usw.). \\
Aus den grundsätzlich einwertigen Features werden dann mehrwertige durch Histogrammerstellung gebildet. Hierbei sind die Grenzen zusätzlich vom Scale abhängig, da die Werte im Allgemeinen eine höhere Varianz im kleinen Scale besitzen. So können im Radius von wenigen Zentimetern kleine Unterschiede einen großen Einfluss auf den Wert haben, was auch an der geringen Zahl an Nachbarn liegt (siehe Tabelle X). Für Scales von 10, 20 und mehr Zentimetern gleicht sich dies für die allermeisten Punkte wieder aus und die Werte nähern sich dem Durchschnitt der gesamten Punktwolke an. Für einwertige Features mit eher niedrigen Werten (Linearity, Scattering, Curvature) wird jeweils die Obergrenze der Histogramme angepasst, für Planarity mit eher hohen Werten entsprechend die Untergrenze. Die genauen Kennzahlen der Histogramme, die vor allem durch manuelle Inspektion für eine gute Unterscheidung zwischen den Klassen festgelegt wurden, finden sich in Tabelle \ref{table:eigenvalue_features_impl}.

\begin{table}
\centering
\begin{tabular}{c|c|c|c|c|c|c|c}
Feature & Binzahl & Feste Grenze & 3$cm$ & 6$cm$ & 9$cm$ & 12$cm$ & 15$cm$ \\
\hline
Linearity  & XXX & 0.0 & XXX & XXX & XXX & XXX & XXX \\
Planarity  & XXX & 1.0 & XXX & XXX & XXX & XXX & XXX \\
Scattering & XXX & 0.0 & XXX & XXX & XXX & XXX & XXX \\
Curvature  & XXX & 0.0 & XXX & XXX & XXX & XXX & XXX 
\end{tabular}
\caption{Die Kennzahlen der linearen Histogramme von den Eigenwert-basierten Features. Ein Eintrag von 0.0 in der Spalte \textit{Feste Grenze} bedeutet, dass in den folgenden Spalten die Obergrenze angepasst wird. Ein Eintrag von 1.0 bedeutet entsprechend Gegenteiliges.}
\label{table:eigenvalue_features_impl}
\end{table}

\subsection{Scales} 

Um trotz der oftmals Millionen von Punkten einer Punktwolke schnell positionelle Beziehungen herzustellen, wozu etwa das Finden der Nachbarschaft eines Punktes gehört, bedarf es dafür geeigneter Datenstrukturen. Eine solche ist auch der \textit{k-d-Baum}, der einen effizienten Zugriff auf multidimensionale Daten erlaubt, wie es die 3D-Punktkoordinaten sind. Das PCTool bietet dafür die Klasse \textit{SpatialSearch} an, die intern einen k-d-Baum implementiert und eine einfach zu nutzende Schnittstelle bietet. So können mit jeweils einem Funktionsaufruf sowohl die k nächsten Nachbarn einer 3D-Position oder eines Punktindizes gefunden werden als auch alle Nachbarn in einem vorgegebenen Scale. Um einen Eindruck von den durchschnittlichen Nachbarzahlen verschiedener Scales zu erhalten, sind diese in Tabelle X gerundet aufgelistet für die Trainingspunktwolke, jeweils auch separat für den Trajektorie-Teil und den Rest. Wie den Daten zu entnehmen ist und wie bereits angesprochen, steigen die Zahlen immer schneller an für höhere Scales. Die teils erheblich unterschiedlichen Nachbarzahlen auch im selben Scale machen ferner deutlich, warum nur die Frequencies der Histogramme - also relative Werte - Bedeutung für die letztlichen Featurevektoren haben. Mit Blick auf die für diesen Ansatz relevanten Klassen stellt sich die Frage, welche Scales sinnvoll und nötig sind. \\\\
Für die unteren Scales wird hier die Grenze bei 3$cm$ angesetzt. Dies ist gering genug für die Feinheiten der Flickstellen oder potenziell sehr kleine Schlaglöcher. Um aussagekräfige Features aus Nachbarschaften zu extrahieren und nicht zu stark von Messungenauigkeiten beeinflusst zu werden, benötigt es eine gewisse Grundanzahl an Nachbarn. Die noch kleineren Scales erfüllen dies nicht und werden daher nicht verwendet. Bei den größeren Scales muss abgewogen werden zwischen ihrem zusätzlichen Informationsgehalt und der Verarbeitungsdauer. Den wohl meisten Nutzen aus jenen Scales ziehen die mittigen Punkte der größeren Gullys: Durch ihre lokale Ebenheit wird die Zugehörigkeit zu einer anderen Klasse erst bei Betrachtung weiter entfernter Nachbarn potenziell erkennbar. Allerdings genügt auch für diese Gullys ein Scale von etwa 25$cm$ von der Mitte bis zum charakteristischen Innenring, weshalb dies die naheliegende obere Schranke darstellt. Da zumindest diese Arbeit einen stärkeren Fokus auf die Schadenserkennung legt als auf die vollständige Identifikation von Gullys, liegt die obere Schranke für die standardmäßigen Versuche bei nur 15$cm$. Im Kapitel \textit{Evaluierung} findet sich auch ein Vergleich zu einem Experiment mit zusätzlichem 20$cm$-Scale. In der Range von 3 bis 15$cm$ wird nun jeder Scale im Abstand von wiederum 3$cm$ für die Featuregewinnung genutzt. Dies soll einerseits eine angemessene Menge von Scales darstellen, andererseits typische Größen von anzufindenden Schlaglöchern repräsentieren.
% Tabelle um Werte ergänzen, ggf. sogar getrennt in jeweils Trajektorie und Rand (vllt. bei Straßenextraktion für Begründung der zwei Teile auch hierauf verweisen)

\subsection{Ein- und mehrwertige Features} 

\mediumtodo{diese Klassenstruktur aktuell noch nicht umgesetzt, aber einfach und die Tage kommend}
Im Zuge dieser Arbeit wurde im PCTool eine eigene Histogramm-Datenstruktur implementiert. Das geschah zum einen aus Mangel an Alternativen, zum anderen, um für den Feature-Extraction-Ansatz nötige Methoden wie das Distanzmaß oder die Berechnung einer durchschnittlichen Repräsentation (siehe Abschnitt \textit{Uniqueness}) direkt einzubauen. Für letztere werden die Counts der einzelnen Bins entsprechend erhöht (ggf. mit Gewichtung) und die Frequencies aktualisiert. Ansonsten werden erwartbare Funktionalitäten bereitgestellt wie die Rückgabe eben dieser Frequencies. \\
Zum Zwecke der Erweiterbarkeit und optimierter Speichernutzung ist \texttt{Histogram} eine abstrakte Klasse, die nur die Counts speichert. Wie bereits erwähnt, kann ein Histogramm linear oder nicht-linear sein, je nachdem, ob dessen Bins einen kontinuierlichen und gleichmäßig aufgeteilten Wertebereich aufspannen. Diese Arten werden in den beiden Unterklassen \texttt{LinearHistogram} und \texttt{NonLinearHistogram} umgesetzt mit ihren entsprechenden internen Arbeitsweisen. In diesem Ansatz wurden keine nicht-linearen Histogramme genutzt, was insbesondere an ihrer verhältnismäßig aufwändigen Berechnung liegt: Statt die Bins direkt in einer Iteration der Daten zu befüllen, ist eine vorherige Sortierung dieser nötig. In Verbindung mit größeren Scales macht sich die $nlog(n)$-Komplexität in der Laufzeit bemerkbar. \\ 
Die dritte Unterklasse ist das \texttt{AbsoluteHistogram}. Dieses speichert (mehrere) einwertige Features anstatt echter Bins, wobei keine Counts, sondern nur die Frequencies existieren. Es ist also der grundlegenden Bedeutung nach kein Histogramm, seine Frequencies müssen summiert auch nicht 1 ergeben. Für diesen Ansatz liegen die einwertigen Features aber alle in der Range von 0 bis 1. Es ist vornehmlich eingeführt worden für den einheitlichen Umgang mit ein- und mehrwertigen Features bezüglich der Speicherung und des Distanzmaßes, obwohl die Semantik eine andere ist. \\
Bedingt durch die unterschiedlichen Klassen können, wie weiter oben geschildert, die Werte wie etwa Curvature in einem Scale deutlich zwischen den Punkten variieren. Da dennoch Minima und Maxima in den Histogrammen abgebildet werden sollen, hat dies - auch wegen der Nutzung linearer Histogramme - teilweise viele leere Bins zur Folge, d.h. Bins mit einem Count und demzufolge einer Frequency von 0. Um während der Featuregewinnung in der PCTool-Node etwas Speicher zu sparen, werden solche Folgen von leeren Bins verkürzt abgespeichert: Statt $k$ ($k \geq 2$) konsekutiver $0.0$-Einträge wird ein Paar geschrieben aus Erkennungssymbol für diese Verkürzung (etwa \texttt{infinity}) und dem $k$ selbst. Beim abschließenden Schreiben der Featuresdatei können die Featurevektoren von Interesse dann wieder leicht in ihre ursprüngliche Größe überführt werden. Um ferner etwas Noise zu reduzieren, der zum Beispiel durch einen einzelnen Nachbarn verursacht wurde, wird ein Schwellwert von $0.005$ genutzt statt des festen Werts von $0.0$, um als ``leerer'' Bin zu gelten. Zwar sorgt dies dafür, dass die Summe aller Frequencies eines Histogramms in den Featurevektoren nicht mehr unbedingt gleich 1 sein muss; dies führte allerdings zu keinen signifikanten Änderungen der Predictions.
% evtl. Klassendiagramm als Bild, aber eig eher unnötig und klein -> doch lieber irgendwo ein Bild eines richtigen Histogramms (vllt. mein Featurehist für einen Scale, damit noch übersichtlich)

\subsection{Uniqueness} 

Die Meanrepräsentation eines Scales, also der Durchschnitt der Featurerepräsentationen im jeweiligen Scale von allen Punkten, wird ungewichtet berechnet durch die Meanmethode der Histogram-Klasse. Das genutzte Distanzmaß zwischen den Featurerepräsentationen ist die \textit{Kullback-Leibler-Divergenz (KL-Divergenz)} \mediumtodo{Paper?}. Dieses ist originär ein nicht-kommutatives Maß über zwei Wahrscheinlichkeitsverteilungen $P$ und $Q$ und drückt den Grad ihrer Verschiedenheit aus. In diesem Ansatz werden keine Wahrscheinlichkeitsverteilungen in jenem Sinne verwendet, doch die mehrwertigen Featureräume in Form von Histogrammen erfüllen deren nötige Eigenschaften: Alle Einträge, in diesem Fall die Frequencies der Bins, liegen zwischen 0 und 1 und ihre Gesamtsumme beträgt genau 1. Für einwertige Featureräume ist die zweite Eigenschaft nicht erfüllt, sie machen aber nur einen kleinen Teil der vollständigen Featurevektoren aus und werden aus diesem Grund dennoch identisch behandelt. \cite{Rusu.etal-2008} nutzen die KL-Divergenz ebenfalls als eines von mehreren getesteten Distanzmaßen. $P$ steht hierbei jeweils für die Featurerepräsentation eines Punktes, $Q$ immer für die Meanrepräsentation eines Scales.
Die Formel zur Berechnung der Distanz zwischen diesen Featurerepräsentationen, wobei $P(i)$ und $Q(i)$ die Frequencies der $i$-ten Bins und $n$ die Binzahl bezeichnen, lautet:
\begin{equation}    
D(P || Q) = \sum_{i=1}^{n}{(P(i) - Q(i)) * \log_2{\frac{P(i)}{Q(i)}}}
\end{equation}
Die KL-Divergenz ist nur definiert, wenn für alle Einträge $i$ mit $Q(i)=0$ auch $P(i)=0$ gilt. Dies ist hier der Fall, da ein Eintrag von 0 im Mean immer bedeutet, dass der entsprechende Eintrag auch bei der Repräsentation jedes Punktes gleich 0 ist. Für $P(i)=0$ ist der Summand als 0 definiert. Von der Liste aller Distanzen werden daraufhin der Durchschnitt sowie die Standardabweichung berechnet. Alle Punkte mit einer Distanz größer-gleich dem Durchschnitt plus \texttt{alpha}-Mal der Standardabweichung gelten als unique. Dabei ist \texttt{alpha} ein einstellbarer Parameter und je größer dieser ist, desto vorsichtiger und eindeutiger verläuft die Einstufung als unique. In den Experimenten hat sich generell ein Wert von 1.0 als annehmbar dargestellt. \\
Damit bei sehr unterschiedlichen Binzahlen der verschiedenen Featureräume - wenn etwa Curvature doppelt so viele Bins hätte wie die Intensitätsdifferenzen - kein Featureraum einen zu starken Einfluss auf die Entscheidung zur Uniqueness hat, wird letztere mit obigem Verfahren separat für jeden Featureraum berechnet. Sobald ein Punkt in mindestens einem Featureraum als unique gilt, tut er dies für den gesamten Scale. Dieses Prinzip ist vor allem mit Blick auf potenzielle zukünftige Erweiterungen eingeführt worden, sollten zum Beispiel neue Features hinzukommen oder sich die Kennzahlen der Histogramme ändern.

\subsection{Approximation für größere Scales} 

Bei zu großen Scales wird auf der Punktwolke eine Density Reduction durchgeführt. Dazu wird intern eine Kopie der Originalpunktwolke angelegt, die sogenannte \textit{Half Edge Length} bestimmt, welche die Stärke der Dichtereduktion festlegt, und letztere entsprechend vorgenommen. Auf dieser reduzierten Punktwolke werden nun ebenfalls ein k-d-Baum für den effizienten Zugriff erzeugt sowie die Features, wie gehabt, berechnet. Der Schwellwert für die Operation liegt bei 20$cm$. Für kleinere Scales ist der Mehraufwand mit der Kopie, der Dichtereduktion und der gleich erklärten Approximation zu groß im Vergleich zur standardmäßigen Prozessierungszeit ohne Density Reduction. \\
Die für die reduzierte Anzahl an Punkten berechneten Features müssen schließlich auf die originale Menge zurück überführt werden. Der Ansatz ist hierbei der folgende: Für jeden Originalpunkt $P$ wird dessen Nachbarschaft $\mathfrak{N}$ in der reduzierten Punktwolke bestimmt. Der Scale für die Suche beträgt 3$cm$, um keine zu weit entfernten Punkte und somit Featurevektoren einzubeziehen. Für den Ausnahmefall, dass sich kein einziger Punkt in diesem Scale befindet, wird lediglich der nächste Punkt (kNN = 1) herangezogen. Da nähere Punkte eher den tatsächlichen Featurevektor von $P$ für diesen Scale repräsentieren, sollen sie auch stärker bei der Approximation gewichtet werden. Dies geschieht über die inverse Distanz zu $P$, wobei dieser Wert - insbesondere, damit die einwertigen Features im Wertebereich von 0 bis 1 verbleiben - noch durch die Summe aller inversen Distanzen normalisiert wird:
\begin{equation}
    \omega_i = \nicefrac{\frac{1}{d(P, \mathfrak{N}_i)}}{\sum_{j = 1}^{|\mathfrak{N}|}{\frac{1}{d(P, \mathfrak{N}_j)}}}, i \in \{1, ..., |\mathfrak{N}|\}
\end{equation}
Dabei entspricht $d$ der Distanzfunktion im euklidischen Raum. Mithilfe der Gewichtungen und der Featurevektoren $\mathfrak{F}$ der Nachbarschaft kann schließlich der Featurevektor $F$ von $P$ als gewichteter Durchschnitt berechnet werden \citep{Rusu.etal-2009}, umgesetzt durch die \textit{Mean}-Methode der \textit{Histogram}-Klasse:
\begin{equation}
    F_P = \sum\limits_{i = 1}^{|\mathfrak{N}|}{\omega_i * \mathfrak{F}_{\mathfrak{N}_i}}
\end{equation}

\subsection{Prediction per Random Forest} 

Für die Predictions mittels Random Forest wird das in Python wohlbekannte Paket \textit{scikit-learn} verwendet. Dieses bietet vielseitige, einfach zu bedienende Funktionalitäten im Bereich Machine Learning. Dazu zählen neben Modellen verschiedener Art auch Funktionen zum Berechnen von Qualitätsmetriken solcher trainierter Modelle. \\
Die Python-Skripte des Ansatzes lassen sich über ein Kommandozeilen-Interface bedienen:
\begin{itemize}
    \item Es kann ein Random Forest erstellt und trainiert werden. Dazu muss eine Featuresdatei eingelesen werden sowie eine Punktwolke mit den Ground-Truth-Klassen. Darauf kann das Modell dann \textit{gefittet} werden.
    \item Es kann ein Random Forest getestet werden auf seine Qualität. Dazu muss neben dem trainierten Modell erneut eine Featuresdatei eingelesen werden sowie eine Punktwolke mit den Ground-Truth-Klassen. Anschließend predictet der Random Forest die Klassen der entsprechenden Punkte, was mit den tatsächlichen abgeglichen und in Metriken pro Klasse ausgedrückt wird. Zu den hier verwendeten Metriken siehe Kapitel \textit{Evaluierung}.
    \item Es können die Klassen einer noch nicht annotierten Punktwolke predictet werden. Dazu muss neben dem gewünschten Modell die entsprechende Featuresdatei eingelesen werden sowie die Punktwolke zum Schreiben der Klassen. Dieser Teil ist für den regelmäßigen Einsatz auf der Plattform der bedeutsamste.
\end{itemize}
Wie im zugehörigen Abschnitt des Kapitels \textit{Konzepte} geschildert, lässt sich ein Random Forest an einigen Stellen parametrisieren. Die einzigen beiden Parameter, die für diesen Ansatz manuell gesetzt wurden, sind die Gesamtzahl der Decision Trees, aus denen der Random Forest besteht, sowie die maximale Tiefe pro Baum. Die Anzahl an \textit{Estimators}, wie es bei scikit-learn heißt, liegt für die Experimente bei 100. Es wurden auch höhere Zahlen getestet, die aber keine signifikanten Steigerungen bei der Qualität der Predictions brachten. Während dieser Wert dem von \cite{Zhiqiang.etal-2019} gleicht, liegt die maximale Tiefe eines Baums hier bei 20 statt bei 10. Dies kann damit begründet werden, dass im vorliegenden Ansatz deutlich größere Featurevektoren genutzt werden: statt 48 beinhalten sie hier XXX Werte. Eine noch größere Tiefe brachte bei Versuchen allerdings keine Verbesserungen mehr - im Gegenteil scheint dann der Random Forest zu stark zu overfitten auf die Trainingsdaten und die individuellen Entscheidungen pro Baum zu spezialisiert zu treffen. Generell ist aber festzuhalten, dass der Fokus nicht auf einer detaillierten Evaluierung des Random Forests und dessen Parameter lag, sondern eher auf dem Finden von aussagekräftigen Features.
% genaue Zahlen im Paper nachlesen bzw. zum Schluss, wenn bei mir alle Features feststehen
% ob alle Trainingsdaten für jeden Baum oder einen der anderen Ansätze (Hälfte Boden) -> später ergänzen SOWIE auch hier evtl. Parameter nochmal ändern

\section{Ansatz Deep Learning} 

Da auf die Funktionsweise von PointNet hier kein Einfluss geübt werden kann, beschränken sich die Experimente, deren Ergebnisse im Kapitel \textit{Evaluierung} vorgestellt werden, auf das Nutzen verschiedener Parameterkombinationen. Die letztlich für die Klassifizierung relevanten Parameter des Modells bzw. für dessen Einsatz durch PCNN sind\footnote{https://gitlab.hpi3d.de/pcr/pcnn-docs/-/blob/main/latex/pcnn.pdf}:
\begin{enumerate}
    \item der \textit{Query Radius}. Dieser Wert gibt an, wie weit ein Punkt der Nachbarschaft maximal entfernt sein darf vom Ursprungspunkt. Standardmäßig auf 3$m$ gesetzt für größere Objekte wie Häuser und Bäume, sollte er für den Anwendungsfall dieser Arbeit entsprechend reduziert werden. Da nur ein fester Wert einstellbar ist, werden Experimente durchgeführt für jeden der im Feature-Extraction-Ansatz genutzten Scales. In der Evaluierung wird dabei auch auf die Unterschiede in der jeweiligen Klassifikationsleistung eingegangen. 
    \item die \textit{Neighborhood Size}. Hiermit wird bestimmt, wie groß jede betrachtete Nachbarschaft ist. Weil auch dieser Wert für alle Punkte gleich bleibt, aber nicht jede Nachbarschaft dieselbe Punktanzahl enthält, muss ggf. aufgefüllt werden durch Mehrfachnutzung einzelner Punkte. Damit diese Ausnahmebehandlung nicht allzu häufig Anwendung findet, orientiert sich die genutzte Nachbarschaftsgröße in den Experimenten am jeweiligen Query Radius. Genauer bedeutet dies, dass der Wert nahe der durchschnittlichen Nachbarzahl für den entsprechenden Scale liegt.
    \item der \textit{Length Divider}. Dieser PCNN-eigene Parameter hat einen Einfluss darauf, wie viele Samplepunkte aus der Punktwolke gezogen werden. Je kleiner der Wert, desto mehr Punkte werden betrachtet und umgekehrt. Der für die Experimente genutzte Wert XXX
    \item die Anzahl an Epochen. Eine Epoche ist dann beendet, wenn alle Trainingsdaten das Modell einmal durchlaufen haben. Mit mehr Epochen steigt grundsätzlich die Genauigkeit des Modells, die mit wenigen Epochen noch eher zufällig ist. Allerdings muss dabei beachtet werden, dass zu viele Epochen Overfitting verursachen können. Bemerkbar macht sich dies dann in geringeren Genauigkeiten bei neuen Daten und letztlich schlechteren Predictions. Für die Experimente wurde eine Anzahl von XXX
\end{enumerate} 
% ggf. noch sampling method erwähnen

\section{Postprocessing} 

Das Postprocessing wird in zwei Iterationen ausgeführt, wobei in beiden jeweils eine Punktenachbarschaft im Scale von 7,5$cm$ betrachtet wird. Auch hier wird dazu ein zuvor für die Punktwolke ersteller k-d-Baum genutzt, der an dieser Stelle entsprechend eine Python-Implementierung besitzt. \\
Zunächst wird über die als einfache Straße klassifizierten Punkte iteriert und der Prozentsatz an Punkten im Scale ermittelt, der nicht Straße ist. Übersteigt dieser die Marke von 50\%, wird dem Ursprungspunkt die häufigste Nicht-Straßenklasse seiner Nachbarschaft zugeteilt. Dies soll Punkte etwa im Inneren von Schlaglöchern oder Gullys ausbessern, die fälschlicherweise als Straße klassifiziert wurden, ggf. weil nicht als unique markiert. Für eine weitere Zeitersparnis wird diese Iteration nur auf den Straßenpunkten ausgeführt, die im obigen Scale mindestens eines Nicht-Straßenpunktes liegen, da auch nur jene für diese Erweiterung interessant sind. Die Ermittlung dieser Straßenpunkte kann zuvor erfolgen und ist deshalb effizient, da die Menge der Straßenpunkte die der Nicht-Straßenpunkte in mehreren Größenordnungen übersteigt. \\
Die zweite Iteration zielt darauf ab Außenseiterpunkte, die nicht als Straße klassifiziert wurden, zu entfernen, d.h. ihnen die Klasse Straße zu geben. Dies wird genau dann durchgeführt, wenn im Scale nicht mindestens 10\% der Punkte dieselbe Klasse wie der Ursprungspunkt besitzen. Dies funktioniert gut für Klassen, die wie der Scale auch eher rund sind, darunter Gullys und Schlaglöcher. Für Flickstellen kann dies problematischer sein, da diese eher dünn und linienartig verlaufen. Außerdem ist die Erkennung von Flickstellen - wie im Kapitel \textit{Evaluierung} nachzulesen - teilweise unstet, was dieser Methode der Nachverarbeitung nicht zugutekommt. Daher werden die als Flickstelle klassifizierten Punkte in dieser Iteration übersprungen. \\
Wie im zugehörigen \textit{Konzepte}-Abschnitt erläutert, ist dieses heuristische Postprocessing eher vorsichtig gewählt und soll vermeintlich eindeutige Fehlklassifzierungen ausbessern. Die ursprünglichen Klassifizierungen haben jedoch, ob beim Feature-Extraction-Ansatz oder bei PointNet, eine tiefergehende Begründung und letztlich Legitimation.